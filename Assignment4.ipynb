{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0068a3",
   "metadata": {},
   "source": [
    "ASSIGNMENT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d21b9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3bed4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_json(\"C:/Users/tejas/OneDrive/Desktop/Assignments/COMPSCI 753/goodreads_reviews_historybio_train.json\", lines=True)\n",
    "test_data = pd.read_json(\"C:/Users/tejas/OneDrive/Desktop/Assignments/COMPSCI 753/goodreads_reviews_historybio_test.json\", lines=True)\n",
    "val_data = pd.read_json(\"C:/Users/tejas/OneDrive/Desktop/Assignments/COMPSCI 753/goodreads_reviews_historybio_val.json\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571018e",
   "metadata": {},
   "source": [
    "Task 1 - Calculate the global bias, user specific bias and item specific bias on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449fd9e5",
   "metadata": {},
   "source": [
    "(A) Report the global bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dea83e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global bias: 3.7669762808387413\n"
     ]
    }
   ],
   "source": [
    "global_bias = train_data['rating'].mean()\n",
    "print(\"Global bias:\", global_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ecf4d",
   "metadata": {},
   "source": [
    "(B) Report the user specific bias of user id= “3913f3be1e8fadc1de34dc49dab06381”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b49f1d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-specific bias for user id = 3913f3be1e8fadc1de34dc49dab06381 : -0.1139150563489455\n"
     ]
    }
   ],
   "source": [
    "user_id = \"3913f3be1e8fadc1de34dc49dab06381\"\n",
    "user_bias = train_data[train_data['user_id'] == user_id]['rating'].mean() - global_bias\n",
    "print(\"User-specific bias for user id =\", user_id, \":\", user_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef086e5b",
   "metadata": {},
   "source": [
    "(C) Report the item specific bias of book id = “16130”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f1c8590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-specific bias for book id = 16130 : 0.4562653093753264\n"
     ]
    }
   ],
   "source": [
    "book_id = 16130\n",
    "item_bias = train_data[train_data['book_id'] == book_id]['rating'].mean() - global_bias\n",
    "print(\"Item-specific bias for book id =\", book_id, \":\", item_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17478bb0",
   "metadata": {},
   "source": [
    "Task 2 - Implement the regularized latent factor model\n",
    "without bias using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7141775",
   "metadata": {},
   "source": [
    "(A) Implement the regularized latent factor model without considering the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec2340d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, RMSE: 4.456335932717587\n",
      "Epoch 2/10, RMSE: 3.9524613388510756\n",
      "Epoch 3/10, RMSE: 3.6863294126107085\n",
      "Epoch 4/10, RMSE: 3.3672032569617394\n",
      "Epoch 5/10, RMSE: 3.036559279113996\n",
      "Epoch 6/10, RMSE: 2.748625750811755\n",
      "Epoch 7/10, RMSE: 2.506139704724196\n",
      "Epoch 8/10, RMSE: 2.3010319089911686\n",
      "Epoch 9/10, RMSE: 2.126057000024525\n",
      "Epoch 10/10, RMSE: 1.9757311913745244\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "k = 8 \n",
    "learning_rate = 0.01\n",
    "lambda1 = 0.3\n",
    "lambda2 = 0.3\n",
    "epochs = 10\n",
    "\n",
    "user_id_to_index = {user_id: idx for idx, user_id in enumerate(train_data['user_id'].unique())}\n",
    "book_id_to_index = {book_id: idx for idx, book_id in enumerate(train_data['book_id'].unique())}\n",
    "\n",
    "# Initialize matrices P and Q \n",
    "num_users = len(user_id_to_index)\n",
    "num_items = len(book_id_to_index)\n",
    "\n",
    "P = np.random.normal(size=(num_users, k))\n",
    "Q = np.random.normal(size=(num_items, k))\n",
    "\n",
    "# SGD\n",
    "for epoch in range(epochs):\n",
    "    total_error = 0\n",
    "\n",
    "    for _, row in train_data.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        book_id = row['book_id']\n",
    "        rating = row['rating']\n",
    "\n",
    "        # Convert user_id and book_id to indices\n",
    "        i = user_id_to_index[user_id]\n",
    "        j = book_id_to_index[book_id]\n",
    "\n",
    "        # Prediction\n",
    "        prediction = np.dot(Q[j], P[i])\n",
    "\n",
    "        # Error\n",
    "        error = rating - prediction\n",
    "        total_error += error ** 2\n",
    "\n",
    "        # Update P and Q using SGD\n",
    "        P[i] += learning_rate * (error * Q[j] - lambda1 * P[i])\n",
    "        Q[j] += learning_rate * (error * P[i] - lambda2 * Q[j])\n",
    "\n",
    "    # Calculate RMSE for the epoch\n",
    "    rmse = np.sqrt(total_error / len(train_data['rating']))\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a912e97",
   "metadata": {},
   "source": [
    "(B) Use SGD to train the latent factor model on the training data for different\n",
    "values of k in {4, 8, 16}. For each value of k, train the model for 10 epoches/iterations.\n",
    "Report the RMSE for each value of k on the validation data. Pick the model that results\n",
    "in the best RMSE on the validation set and report its RMSE on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9009895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, k=4, Validation RMSE: 4.218897033133351\n",
      "Epoch 2/10, k=4, Validation RMSE: 3.9745931556423226\n",
      "Epoch 3/10, k=4, Validation RMSE: 3.8475118967128497\n",
      "Epoch 4/10, k=4, Validation RMSE: 3.676404752272193\n",
      "Epoch 5/10, k=4, Validation RMSE: 3.424581718893356\n",
      "Epoch 6/10, k=4, Validation RMSE: 3.1579736694340874\n",
      "Epoch 7/10, k=4, Validation RMSE: 2.917251661430983\n",
      "Epoch 8/10, k=4, Validation RMSE: 2.7079944567897156\n",
      "Epoch 9/10, k=4, Validation RMSE: 2.5258700931223284\n",
      "Epoch 10/10, k=4, Validation RMSE: 2.3660954105671834\n",
      "Epoch 1/10, k=8, Validation RMSE: 4.454090582031795\n",
      "Epoch 2/10, k=8, Validation RMSE: 3.9497254464254636\n",
      "Epoch 3/10, k=8, Validation RMSE: 3.673209598582915\n",
      "Epoch 4/10, k=8, Validation RMSE: 3.3435356956490976\n",
      "Epoch 5/10, k=8, Validation RMSE: 3.01105507763816\n",
      "Epoch 6/10, k=8, Validation RMSE: 2.7250661887691967\n",
      "Epoch 7/10, k=8, Validation RMSE: 2.4853194592133336\n",
      "Epoch 8/10, k=8, Validation RMSE: 2.2829089104302254\n",
      "Epoch 9/10, k=8, Validation RMSE: 2.110251673094692\n",
      "Epoch 10/10, k=8, Validation RMSE: 1.9617913893748509\n",
      "Epoch 1/10, k=16, Validation RMSE: 4.911930128796884\n",
      "Epoch 2/10, k=16, Validation RMSE: 3.8601716789072436\n",
      "Epoch 3/10, k=16, Validation RMSE: 3.4459052468329303\n",
      "Epoch 4/10, k=16, Validation RMSE: 3.077368082017259\n",
      "Epoch 5/10, k=16, Validation RMSE: 2.727047389753968\n",
      "Epoch 6/10, k=16, Validation RMSE: 2.430731135084617\n",
      "Epoch 7/10, k=16, Validation RMSE: 2.1880537419897164\n",
      "Epoch 8/10, k=16, Validation RMSE: 1.988792771989589\n",
      "Epoch 9/10, k=16, Validation RMSE: 1.823679497740445\n",
      "Epoch 10/10, k=16, Validation RMSE: 1.68560769668982\n",
      "\n",
      "Best Model - Test RMSE: 3.3249146277557693\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "lambda1 = 0.3\n",
    "lambda2 = 0.3\n",
    "epochs = 10\n",
    "\n",
    "# Values of k to try\n",
    "k_values = [4, 8, 16]\n",
    "\n",
    "best_model = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for k in k_values:\n",
    "    # Create mappings for user_id and book_id to indices\n",
    "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(train_data['user_id'].unique())}\n",
    "    book_id_to_index = {book_id: idx for idx, book_id in enumerate(train_data['book_id'].unique())}\n",
    "\n",
    "    # Initialize matrices P and Q\n",
    "    num_users = len(user_id_to_index)\n",
    "    num_items = len(book_id_to_index)\n",
    "    P = np.random.normal(size=(num_users, k))\n",
    "    Q = np.random.normal(size=(num_items, k))\n",
    "\n",
    "    # SGD\n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "\n",
    "        for _, row in train_data.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            book_id = row['book_id']\n",
    "            rating = row['rating']\n",
    "\n",
    "            # Convert user_id and book_id to indices\n",
    "            i = user_id_to_index[user_id]\n",
    "            j = book_id_to_index[book_id]\n",
    "\n",
    "            # Prediction\n",
    "            prediction = np.dot(Q[j], P[i])\n",
    "\n",
    "            # Error\n",
    "            error = rating - prediction\n",
    "            total_error += error ** 2\n",
    "\n",
    "            # Update P and Q using SGD\n",
    "            P[i] += learning_rate * (error * Q[j] - lambda1 * P[i])\n",
    "            Q[j] += learning_rate * (error * P[i] - lambda2 * Q[j])\n",
    "\n",
    "        # Calculate RMSE for the epoch on validation data\n",
    "        val_rmse = np.sqrt(total_error / len(train_data))  # Adjusted this line\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, k={k}, Validation RMSE: {val_rmse}\")\n",
    "\n",
    "    # If this model has the best RMSE on the validation set, update the best model\n",
    "    if val_rmse < best_rmse:\n",
    "        best_rmse = val_rmse\n",
    "        best_model = (P.copy(), Q.copy())\n",
    "\n",
    "    # Calculate RMSE for the test data after all epochs\n",
    "    total_error_test = 0\n",
    "    num_test_samples = 0\n",
    "\n",
    "    for _, row in test_data.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        book_id = row['book_id']\n",
    "        rating = row['rating']\n",
    "\n",
    "    # Check if user_id and book_id exist in the dictionary\n",
    "        if user_id in user_id_to_index and book_id in book_id_to_index:\n",
    "            # Convert user_id and book_id to indices\n",
    "            i = user_id_to_index[user_id]\n",
    "            j = book_id_to_index[book_id]\n",
    "\n",
    "        # Prediction\n",
    "            prediction = np.dot(Q[j], P[i])\n",
    "\n",
    "        # Error\n",
    "            error = rating - prediction\n",
    "            total_error_test += error ** 2\n",
    "            num_test_samples += 1\n",
    "\n",
    "# Calculate RMSE for the test data\n",
    "test_rmse = np.sqrt(total_error_test / num_test_samples) if num_test_samples > 0 else float('inf')\n",
    "print(f\"\\nBest Model - Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297f7d9",
   "metadata": {},
   "source": [
    "Task 3 - Implement the regularized latent factor model\n",
    "with bias using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2207a94",
   "metadata": {},
   "source": [
    "(A) Incorporate the bias terms bg, b(user)i and b(item)j to the latent factor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e59521a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, RMSE: 3.551357939080106\n",
      "Epoch 2/10, RMSE: 2.4030778255372804\n",
      "Epoch 3/10, RMSE: 1.9805711925485754\n",
      "Epoch 4/10, RMSE: 1.7396813492496281\n",
      "Epoch 5/10, RMSE: 1.584403212028687\n",
      "Epoch 6/10, RMSE: 1.4773995163558475\n",
      "Epoch 7/10, RMSE: 1.4001880225210526\n",
      "Epoch 8/10, RMSE: 1.3424124063671823\n",
      "Epoch 9/10, RMSE: 1.297822498128653\n",
      "Epoch 10/10, RMSE: 1.2624550166995512\n",
      "\n",
      "Learned user-specific bias for 3913f3be1e8fadc1de34dc49dab06381: 2.5096203616916104\n",
      "Learned item-specific bias for 16130: 2.382003855945663\n"
     ]
    }
   ],
   "source": [
    "k = 8\n",
    "learning_rate = 0.01\n",
    "lambda1 = 0.3\n",
    "lambda2 = 0.3\n",
    "lambda3 = 0.3\n",
    "lambda4 = 0.3\n",
    "epochs = 10\n",
    "\n",
    "# Create mappings for user_id and book_id to indices\n",
    "user_id_to_index = {user_id: idx for idx, user_id in enumerate(train_data['user_id'].unique())}\n",
    "book_id_to_index = {book_id: idx for idx, book_id in enumerate(train_data['book_id'].unique())}\n",
    "\n",
    "# Initialize matrices P and Q\n",
    "num_users = len(user_id_to_index)\n",
    "num_items = len(book_id_to_index)\n",
    "P = np.random.normal(size=(num_users, k))\n",
    "Q = np.random.normal(size=(num_items, k))\n",
    "\n",
    "# Initialize bias terms\n",
    "user_bias = {user_id: 0 for user_id in train_data['user_id'].unique()}\n",
    "item_bias = {book_id: 0 for book_id in train_data['book_id'].unique()}\n",
    "\n",
    "# SGD\n",
    "for epoch in range(epochs):\n",
    "    total_error = 0\n",
    "\n",
    "    for _, row in train_data.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        book_id = row['book_id']\n",
    "        rating = row['rating']\n",
    "\n",
    "        # Convert user_id and book_id to indices\n",
    "        i = user_id_to_index.get(user_id, -1)\n",
    "        j = book_id_to_index.get(book_id, -1)\n",
    "\n",
    "        # Check for new users or items\n",
    "        if i == -1 or j == -1:\n",
    "            user_bias_i = 0\n",
    "            item_bias_j = 0\n",
    "        else:\n",
    "            user_bias_i = user_bias[user_id]\n",
    "            item_bias_j = item_bias[book_id]\n",
    "\n",
    "        # Prediction\n",
    "        prediction = np.dot(Q[j], P[i]) + user_bias_i + item_bias_j\n",
    "\n",
    "        # Error\n",
    "        error = rating - prediction\n",
    "        total_error += error ** 2\n",
    "\n",
    "        # Update P, Q, user_bias, and item_bias using SGD\n",
    "        P[i] += learning_rate * (error * Q[j] - lambda1 * P[i])\n",
    "        Q[j] += learning_rate * (error * P[i] - lambda2 * Q[j])\n",
    "        user_bias[user_id] += learning_rate * (error - lambda3 * user_bias[user_id])\n",
    "        item_bias[book_id] += learning_rate * (error - lambda4 * item_bias[book_id])\n",
    "\n",
    "    # Calculate RMSE for the epoch\n",
    "    rmse = np.sqrt(total_error / len(train_data))\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, RMSE: {rmse}\")\n",
    "\n",
    "# Report learned biases\n",
    "user_id_to_find = \"3913f3be1e8fadc1de34dc49dab06381\"\n",
    "book_id_to_find = 16130\n",
    "\n",
    "learned_user_bias = user_bias.get(user_id_to_find, \"Not Found\")\n",
    "learned_item_bias = item_bias.get(book_id_to_find, \"Not Found\")\n",
    "\n",
    "print(f\"\\nLearned user-specific bias for {user_id_to_find}: {learned_user_bias}\")\n",
    "print(f\"Learned item-specific bias for {book_id_to_find}: {item_bias.get(book_id_to_find, 'Not Found')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2342037e",
   "metadata": {},
   "source": [
    "(B) Similar to Task 2 (B), find the best k in {4, 8, 16} for the model you devel-\n",
    "oped in Task 3 (A) on the validation set, by using RMSE to compare across these models,\n",
    "and apply the best of these models to the test data. Compare the resulting test RMSE with\n",
    "Task 2 (B). Analyse and explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a134f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, k=4, Validation RMSE: 2.397853277523409\n",
      "Epoch 10/10, k=8, Validation RMSE: 2.189841346103022\n",
      "Epoch 10/10, k=16, Validation RMSE: 1.9654329964601407\n",
      "\n",
      "Best k: 16, Best Validation RMSE: 1.9654329964601407\n",
      "Test RMSE using the best model (k=16): 1.9654329964601407\n"
     ]
    }
   ],
   "source": [
    "#sample_size = 15\n",
    "#train_data = train_data.sample(n=sample_size, random_state=42)\n",
    "#test_data = test_data.sample(n=sample_size, random_state=42)\n",
    "#val_data = val_data.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.01\n",
    "lambda1 = 0.3\n",
    "lambda2 = 0.3\n",
    "lambda3 = 0.3\n",
    "lambda4 = 0.3\n",
    "epochs = 10\n",
    "\n",
    "# Candidate values for k\n",
    "candidate_ks = [4, 8, 16]\n",
    "\n",
    "# Create mappings for user_id and book_id to indices\n",
    "user_id_to_index = {user_id: idx for idx, user_id in enumerate(train_data['user_id'].unique())}\n",
    "book_id_to_index = {book_id: idx for idx, book_id in enumerate(train_data['book_id'].unique())}\n",
    "\n",
    "# Initialize matrices P and Q\n",
    "num_users = len(user_id_to_index)\n",
    "num_items = len(book_id_to_index)\n",
    "\n",
    "best_k = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for k in candidate_ks:\n",
    "    # Initialize matrices P and Q for the current k\n",
    "    P = np.random.normal(size=(num_users, k))\n",
    "    Q = np.random.normal(size=(num_items, k))\n",
    "\n",
    "    # Initialize bias terms\n",
    "    user_bias = {user_id: 0 for user_id in train_data['user_id'].unique()}\n",
    "    item_bias = {book_id: 0 for book_id in train_data['book_id'].unique()}\n",
    "\n",
    "    # SGD\n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "\n",
    "        for _, row in train_data.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            book_id = row['book_id']\n",
    "            rating = row['rating']\n",
    "\n",
    "            # Convert user_id and book_id to indices\n",
    "            i = user_id_to_index.get(user_id, -1)\n",
    "            j = book_id_to_index.get(book_id, -1)\n",
    "\n",
    "            # Check for new users or items\n",
    "            if i == -1 or j == -1:\n",
    "                user_bias_i = 0\n",
    "                item_bias_j = 0\n",
    "            else:\n",
    "                user_bias_i = user_bias[user_id]\n",
    "                item_bias_j = item_bias[book_id]\n",
    "\n",
    "            # Prediction\n",
    "            prediction = np.dot(Q[j], P[i]) + user_bias_i + item_bias_j\n",
    "\n",
    "            # Error\n",
    "            error = rating - prediction\n",
    "            total_error += error ** 2\n",
    "\n",
    "            # Update P, Q, user_bias, and item_bias using SGD\n",
    "            P[i] += learning_rate * (error * Q[j] - lambda1 * P[i])\n",
    "            Q[j] += learning_rate * (error * P[i] - lambda2 * Q[j])\n",
    "            user_bias[user_id] += learning_rate * (error - lambda3 * user_bias[user_id])\n",
    "            item_bias[book_id] += learning_rate * (error - lambda4 * item_bias[book_id])\n",
    "\n",
    "        # Calculate RMSE for the epoch\n",
    "        rmse = np.sqrt(total_error / len(train_data))\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    val_predictions = []\n",
    "\n",
    "    for _, row in val_data.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        book_id = row['book_id']\n",
    "        rating = row['rating']\n",
    "\n",
    "        # Convert user_id and book_id to indices\n",
    "        i = user_id_to_index.get(user_id, -1)\n",
    "        j = book_id_to_index.get(book_id, -1)\n",
    "\n",
    "        # Check for new users or items\n",
    "        if i == -1 or j == -1:\n",
    "            user_bias_i = 0\n",
    "            item_bias_j = 0\n",
    "        else:\n",
    "            user_bias_i = user_bias[user_id]\n",
    "            item_bias_j = item_bias[book_id]\n",
    "\n",
    "        # Prediction\n",
    "        prediction = np.dot(Q[j], P[i]) + user_bias_i + item_bias_j\n",
    "        val_predictions.append(prediction)\n",
    "\n",
    "    val_rmse = np.sqrt(total_error / len(val_data))\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, k={k}, Validation RMSE: {val_rmse}\")\n",
    "\n",
    "    # Update best_k if the current model is better\n",
    "    if val_rmse < best_rmse:\n",
    "        best_rmse = val_rmse\n",
    "        best_k = k\n",
    "\n",
    "print(f\"\\nBest k: {best_k}, Best Validation RMSE: {best_rmse}\")\n",
    "\n",
    "# Use the best model on the test set\n",
    "test_predictions = []\n",
    "\n",
    "for _, row in test_data.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    book_id = row['book_id']\n",
    "    rating = row['rating']\n",
    "\n",
    "    # Convert user_id and book_id to indices\n",
    "    i = user_id_to_index.get(user_id, -1)\n",
    "    j = book_id_to_index.get(book_id, -1)\n",
    "\n",
    "    # Check for new users or items\n",
    "    if i == -1 or j == -1:\n",
    "        user_bias_i = 0\n",
    "        item_bias_j = 0\n",
    "    else:\n",
    "        user_bias_i = user_bias[user_id]\n",
    "        item_bias_j = item_bias[book_id]\n",
    "\n",
    "    # Prediction\n",
    "    prediction = np.dot(Q[j], P[i]) + user_bias_i + item_bias_j\n",
    "    test_predictions.append(prediction)\n",
    "\n",
    "test_rmse = np.sqrt(total_error / len(test_data))\n",
    "print(f\"Test RMSE using the best model (k={best_k}): {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb30ac",
   "metadata": {},
   "source": [
    "The validation RMSE decreases as the value of k increases. This is a common behavior, as a more complex model (higher k) can better fit the training data. The model with k=16 achieves the lowest validation RMSE, indicating that it is the best-performing model among the tested values of k.The test RMSE using the best model (k=16) is reported as 1.9654, which is consistent with the validation RMSE. This consistency suggests that the model is generalizing well to unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
